{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8a3c62-216c-446d-851d-7249cc67787a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {align:left;display:block} \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {align:left;display:block} \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e27c8c-7f53-4e9f-b4da-7e0343fb6313",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP)\n",
    "----\n",
    "\n",
    "**Value Iteration Process with Policy Changes in MDP**\n",
    "\n",
    "We begin with a Markov Decision Process (MDP) where an agent decides whether to invest conservatively (C) or aggressively (A) in a financial portfolio. The objective is to find an optimal policy maximizing long-term rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Defining the MDP Components**\n",
    "\n",
    "**States (S):**\n",
    "\n",
    "- Low Wealth (L)\n",
    "- Medium Wealth (M)\n",
    "- High Wealth (H)\n",
    "\n",
    "**Actions (A):**\n",
    "\n",
    "- Conservative (C)\n",
    "- Aggressive (A)\n",
    "\n",
    "**Transition Probabilities:**\n",
    "\n",
    ">| Current State | Action | Next State Probabilities     |\n",
    "| ------------- | ------ | ---------------------------- |\n",
    "| Low (L)       | C      | 80% Stay in L, 20% Move to M |\n",
    "| Low (L)       | A      | 60% Stay in L, 40% Move to M |\n",
    "| Medium (M)    | C      | 70% Stay in M, 30% Move to H |\n",
    "| Medium (M)    | A      | 50% Stay in M, 50% Move to H |\n",
    "| High (H)      | C      | 90% Stay in H, 10% Drop to M |\n",
    "| High (H)      | A      | 70% Stay in H, 30% Drop to M |\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "- Low Wealth (L): -1\n",
    "- Medium Wealth (M): 3\n",
    "- High Wealth (H): 5\n",
    "\n",
    "**Discount Factor (γ):** 0.9\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6becd-538b-4010-b4ed-c2e03c6c2e1a",
   "metadata": {},
   "source": [
    "### **Step 2: Value Iteration Updates**\n",
    "\n",
    "We initialize values: $V_0(L) = 0$, $V_0(M) = 0$, $V_0(H) = 0$.\n",
    "\n",
    "#### **Iteration 1**\n",
    "\n",
    "Using Bellman’s equation:\n",
    "\n",
    ">$\n",
    "V_1(s) = \\max_a \\left[ R(s) + \\gamma \\sum_{s'} P(s' | s, a) V_0(s') \\right]\n",
    "$\n",
    "\n",
    "For **Low Wealth (L):**\n",
    "\n",
    ">$\n",
    "V_1(L) = \\max \\left[ -1 + 0.9(0.8V_0(L) + 0.2V_0(M)), -1 + 0.9(0.6V_0(L) + 0.4V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    ">$\n",
    "V_1(M) = \\max \\left[ 3 + 0.9(0.7V_0(M) + 0.3V_0(H)), 3 + 0.9(0.5V_0(M) + 0.5V_0(H)) \\right]\n",
    "$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">$\n",
    "V_1(H) = \\max \\left[ 5 + 0.9(0.9V_0(H) + 0.1V_0(M)), 5 + 0.9(0.7V_0(H) + 0.3V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "Since $V_0(L) = V_0(M) = V_0(H) = 0$, the initial values are just the rewards.\n",
    "\n",
    ">$\n",
    "V_1(L) = -1, \\quad V_1(M) = 3, \\quad V_1(H) = 5\n",
    "$\n",
    "\n",
    "#### **Policy Evaluation after Iteration 1**\n",
    "\n",
    "> \\$\n",
    "Q(L, C) = -1 + 0.9(0.8(-1) + 0.2(3)) = -1.18\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(L, A) = -1 + 0.9(0.6(-1) + 0.4(3)) = -0.46\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(M, C) = 3 + 0.9(0.7(3) + 0.3(5)) = 4.539\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(M, A) = 3 + 0.9(0.5(3) + 0.5(5)) = 6.6\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(H, C) = 5 + 0.9(0.9(5) +0.1(3)) = 9.32\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(H,A) = 5 + 0.9(0.7(5) +0.3(3)) = 8.96\n",
    "\\$\n",
    "\n",
    "\n",
    "**Policy at Iteration 1:**\n",
    "- L → Aggressive (A)\n",
    "- M → Aggressive (A)\n",
    "- H → Conservative (C)\n",
    "\n",
    "\n",
    "#### **Iteration 2**\n",
    "\n",
    "Updating $V_2(s)$:\n",
    "\n",
    ">$\n",
    "V_2(L) = \\max \\left[ -1 + 0.9(0.8(-1) + 0.2(3)), -1 + 0.9(0.6(-1) + 0.4(3)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(H) = \\max \\left[ 5 + 0.9(0.9(5) + 0.1(3)), 5 + 0.9(0.7(5) + 0.3(3)) \\right]\n",
    "$\n",
    "\n",
    "Computing the above:\n",
    ">$\n",
    "V_2(L) = = -0.46, \\quad V_2(M) = 6.6, \\quad V_2(H) = 9.32\n",
    "$\n",
    "\n",
    "#### **Policy Evaluation after Iteration 2**\n",
    "\n",
    "Completing the iteration...\n",
    "> \\$\n",
    "Q(L,C) = -1 + 0.9(0.8(-0.46) + 0.2(6.6)) = -0.1432\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(L, A) = -1 + 0.9(0.6(-0.46) + 0.4(6.6)) = 1.1276\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(M, C) = 3 + 0.9(0.7(6.6) + 0.3(9.32)) = 9.6744\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(M, A) = 3 + 0.9(0.5(6.6) + 0.5(9.32)) = 10.164\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(H, C) = 5 + 0.9(0.9(9.32) + 0.1(6.6)) = 13.1432\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(H, A) = 5 + 0.9(0.7(9.32) + 0.3(6.6)) = 12.6536\n",
    "\\$\n",
    "\n",
    "**Policy at Iteration 2:**\n",
    "- L → Aggressive (A)\n",
    "- M → Aggressive (A)\n",
    "- H → Conservative (C)\n",
    "\n",
    "#### **Iteration 3**\n",
    "\n",
    "Updating $V_3(s)$:\n",
    "\n",
    "TODO:\n",
    "\n",
    ">$\n",
    "V_3(L) = \\max \\left[ -1 + 0.9(0.8(-0.46) + 0.2(6.6)), -1 + 0.9(0.6(-0.46) + 0.4(6.6)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_3(M) = \\max \\left[ 3 + 0.9(0.7(6.6) + 0.3(9.32)), 3 + 0.9(0.5(6.6) + 0.5(9.32)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_3(H) = \\max \\left[ 5 + 0.9(0.9(9.32) + 0.1(6.6)), 5 + 0.9(0.7(9.32) + 0.3(6.6)) \\right]\n",
    "$\n",
    "\n",
    "Computing these:\n",
    "\n",
    ">$\n",
    "V_3(L) = 1.127, \\quad V_3(M) = 10.164, \\quad V_3(H) = 13.143\n",
    "$\n",
    "#### **Policy Change Analysis**\n",
    "\n",
    "From **Iteration 2 to Iteration 3**, let’s check the action values to determine if the policy changed.\n",
    "\n",
    "For **Low Wealth (L):**\n",
    "\n",
    ">$\n",
    "Q(L, C) = -1 + 0.9(0.8(1.127) + 0.2(10.164)) = 1.64$\n",
    "\n",
    ">$\n",
    "Q(L, A) = -1 + 0.9(0.6(1.127) + 0.4(10.164)) = 3.267$\n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    ">$\n",
    "Q(M, C) = 3 + 0.9(0.7(10.164) + 0.3(13.143)) = 12.95\n",
    "$\n",
    "\n",
    ">$\n",
    "Q(M, A) = 3 + 0.9(0.5(10.164) + 0.5(13.143)) = 13.488\n",
    "$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">$\n",
    "Q(H, C) = 5 + 0.9(0.9(13.143) + 0.1(10.164)) = 16.56\n",
    "$\n",
    "\n",
    ">$\n",
    "Q(H, A) = 5 + 0.9(0.7(13.143) + 0.3(10.164)) = 16.0\n",
    "$\n",
    "\n",
    "Compare $Q(L, A), Q(L, C)$ and $Q(H, C),  Q(H, A)$, decide the policy updates:\n",
    "\n",
    "- **Low Wealth (L)** → Aggressive (A)\n",
    "- **Medium Wealth (M)** → Aggressive (A)\n",
    "- **High Wealth (H)** → Conservative (C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e5695-a01b-4bcc-909b-582b5e64d068",
   "metadata": {},
   "source": [
    "### Summary: Policy Evolution Over Iterations\n",
    "\n",
    "| State  | Iteration 1 | Iteration 2 | Iteration 3 |\n",
    "|--------|------------|------------|------------|\n",
    "| Low      | Aggressive | Agressive | Aggressive |\n",
    "| Medium   | Aggressive | Aggressive| Aggressive |\n",
    "| High     | Conservative| Conservative| Conservative |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781be752-fb55-4086-87fc-cf09d1cc106c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (334022468.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making in environments with stochastic outcomes. This report analyzes the given MDP case and presents insights into its structure, policy iteration, and value iteration processes.\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Markov Decision Process (MDP) Analysis Report\n",
    "\n",
    "## Introduction\n",
    "Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making in environments with stochastic outcomes. This report analyzes the given MDP case and presents insights into its structure, policy iteration, and value iteration processes.\n",
    "\n",
    "## MDP Components\n",
    "### States (S)\n",
    "- Low Wealth (L)\n",
    "- Medium Wealth (M)\n",
    "- High Wealth (H)\n",
    "\n",
    "### Actions (A)\n",
    "- Conservative (C)\n",
    "- Aggressive (A)\n",
    "\n",
    "### Transition Probabilities\n",
    "| Current State | Action | Next State Probabilities |\n",
    "|--------------|--------|-------------------------|\n",
    "| Low (L)      | C      | 80% Stay in L, 20% Move to M |\n",
    "| Low (L)      | A      | 60% Stay in L, 40% Move to M |\n",
    "| Medium (M)   | C      | 70% Stay in M, 30% Move to H |\n",
    "| Medium (M)   | A      | 50% Stay in M, 50% Move to H |\n",
    "| High (H)     | C      | 90% Stay in H, 10% Drop to M |\n",
    "| High (H)     | A      | 70% Stay in H, 30% Drop to M |\n",
    "\n",
    "### Rewards\n",
    "- Conservative action (C) provides a stable but lower reward.\n",
    "- Aggressive action (A) provides higher potential rewards but also carries more risk.\n",
    "\n",
    "## Policy and Value Iteration\n",
    "The Value Iteration algorithm updates the expected utility for each state until convergence. Policy Iteration, in contrast, iterates between evaluating the current policy and improving it based on the maximum expected reward.\n",
    "\n",
    "## Case Analysis\n",
    "1. **Risk vs. Reward Tradeoff**: The aggressive approach (A) provides a higher chance of moving to High Wealth (H) but also increases the risk of losing wealth.\n",
    "2. **Optimal Strategy**: Depending on the discount factor and initial wealth state, a mixed strategy combining both C and A might be ideal.\n",
    "3. **Long-Term Implications**: Over multiple iterations, the policy stabilizes towards an optimal strategy that maximizes long-term rewards.\n",
    "\n",
    "## Conclusion\n",
    "The MDP model presented effectively captures decision-making in financial investments. The optimal policy derived depends on balancing risk and reward, with long-term value maximization being the key goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b16c9e-9ab5-45c9-ae1b-fe8720919668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
